---
title: "EvalExamples"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(purrr)
library(lubridate)
```

Some examples of model evaluation using results
from a hydrologic model applied to a Sierra watershed

We will also learn a bit more about dealing with dates in R

**lubridate** has some useful functions

```{r simple}

sager = read.table("../Data/sager.txt", header=T)
head(sager)

# add date from the existing columns of day, month, year
sager = sager %>% mutate(date=make_date(year=year, month=month, day=day))

# always start with plotting observed and model
# here's where you can catch "unrealistic" values
# plot
sagerl = sager %>% gather(key="source",value="streamflow",-date,-month,-day,-wy,-wyd,-year)

# basic plot to look at how well streamflow is tracked through time
# seasonality, changes through years..
ggplot(sagerl, aes(date, streamflow, col=source, linetype=source))+geom_line()

# change axis to logarithmic to get a closer look at performance at low values
ggplot(sagerl, aes(date, streamflow, col=source, linetype=source))+geom_line()+scale_y_continuous(trans="log")+labs(y="streamflow mm/day")

# look at it another way - use original data set here
# this tells you about biases but not seasonality or trends through time
ggplot(sager, aes(obs, model))+geom_point()+geom_abline(intercept=0, slope=1, col="red")

# apply some functions to measure performance

source("../R/nse.R")
source("../R/relerr.R")
source("../R/cper.R")
nse(m=sager$model, o=sager$obs)

relerr(m=sager$model, o=sager$obs)*100

cper(m=sager$model, o=sager$obs, weight.nse=0.8)


# try a different time step
# often evaluation changes when you change time scale
# choose the time scale most meaningful to your project
sager_wy = sager %>% group_by(wy) %>% summarize(model=sum(model), obs=sum(obs))

nse(sager_wy$model, sager_wy$obs)
cper(m=sager_wy$model, o=sager_wy$obs, weight.nse=0.8)


# just look at august flow
# imagine we are concerned about low flows causing high stream temperatures
# august might be the most important month
# first sum streamflow by month for each year
tmp = sager %>% group_by(month, year) %>% summarize(model=sum(model), obs=sum(obs))
# now extract august
sager_aug = subset(tmp, month==8)
cor(sager_aug$model, sager_aug$obs)

# turn your evaluation metric into a function
# here's one for correlating annual minimum flow
source("../R/check_minannual.R")
check_minannual(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy)



# try another metric
```

Peformance evaluation  may depend on what parameter set you use

Calibration is picking parameter sets based on performance evaluation

Apply metrics over multiple outputs (generated by running across many parameters sets) - like we've done in our sensitivity analysis work


```{r multiple}
# multiple results - lets say we've run the model for multiple years, each column
# is streamflow for a different parameter set
msage = read.table("../Data/sagerm.txt", header=T)

# lets say we know the start date from our earlier output
msage$date = sager$date
head(msage)
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# and we still have observed data from above
# useful to combine by date to make sure that streamflow and observe match

msage$obs = sager$obs


# how can we plot all results
# to turn all the columns of different outputs into a single column identified by "run"
msagel = msage %>% gather(key="run",value="streamflow", -date, -month, -day, -year, -wy, -obs)

#lets plot water year 1970 otherwise its hard to see
p1=ggplot(subset(msagel, wy == 1970), aes(as.Date(date), streamflow, col=run))+geom_line()+theme(legend.position = "none")
p1
# lets add observed streamflow
p1+geom_line(aes(as.Date(date), obs), size=2, col="black", linetype=2)+labs(y="Streamflow", x="Date")


# compute performance measures for all output
res = msage %>% select(-date, -month, -day, -year, -wy ) %>% map_dbl(~nse(m=.x, o=msage$obs))
summary(res)

# one of them has a "perfect score" why?
# redo
res = msage %>% select(-date, -month, -day, -year, -wy, -obs) %>% map_dbl(~nse(m=.x, o=msage$obs))
summary(res)

# if we want to keep track of which statistics is associated with each run, we need a unique identifies
# a ID that tracks each model output - lets use the column names
simnames = names(msage %>% select(-date, -month, -day,-year,-wy, -obs))
results = cbind.data.frame(simnames=simnames, nse=res)

# another example using our low flow statistics
# use apply to compute for all the data
res = msage %>% select(-date, -month, -day, -year, -wy, -obs ) %>% map_dbl(~check_minannual( o=msage$obs, month=msage$month, day=msage$day, year=msage$year, wy=msage$wy, m=.x))


# add to our results
results$minannual_cor = res


# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(results)

# graph range of performance measures
resultsl = results %>% gather(key="metric",value="value", -simnames)
ggplot(resultsl, aes(metric, value))+geom_boxplot()+facet_wrap(~metric, scales="free")

# are metrics related to each other
# useful for assessing whether there are tradeoffs
ggplot(results, aes(minannual_cor, nse))+geom_point()

```
