---
title: "Evaluating Models"
author: "Lizzy Schattle and Claudia Flores"
date: "5/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
library(lubridate)
library(purrr)
```

Develop your own performance metric.

1. Code your metric as a function
```{r}
sager = read.table("sager.txt", header=T)
#head(sager)

msage = read.table("sagerm.txt", header=T)
#head(msage)

source("rmse.R")
source("check_minannual.R")
```

2. Apply to the streamflow data provided in sagerm.txt (multiple model results)
```{r}
# add date from the existing columns of day, month, year
sager = sager %>% mutate(date=make_date(year=year, month=month, day=day))

# lets say we know the start date from our earlier output
msage$date = sager$date
#head(msage)
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# and we still have observed data from above
# useful to combine by date to make sure that streamflow and observe match
msage$obs = sager$obs

# how can we plot all results
# to turn all the columns of different outputs into a single column identified by "run"
msagel = msage %>% gather(key="run",value="streamflow", -date, -month, -day, -year, -wy, -obs)

# compute performance measures for all output
res = msage %>% select(-date, -month, -day, -year, -wy, -obs) %>% map_dbl(~rmse(m=.x, o=msage$obs))
summary(res)

# if we want to keep track of which statistics is associated with each run, we need a unique identifies
# a ID that tracks each model output - lets use the column names
simnames = names(msage %>% select(-date, -month, -day,-year,-wy, -obs))
results = cbind.data.frame(simnames=simnames, rmse=res)

# another example using our low flow statistics
# use apply to compute for all the data
res = msage %>% select(-date, -month, -day, -year, -wy, -obs ) %>% map_dbl(~check_minannual( o=msage$obs, month=msage$month, day=msage$day, year=msage$year, wy=msage$wy, m=.x))

# add to our results
results$minannual_cor = res
```

3.  Find the simulation that gives the best performance (record that and add to the quiz on gauchospace)
```{r}
# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(results)

# simulation 100.1 has the min number for RMSE
```
Simulation 100.1 has the min number for RMSE.

4. Create a boxplot of your metric applied to sagerm.txt
```{r}
# graph RMSE values
ggplot(results, aes(rmse)) +
  geom_boxplot() +
  labs(x = "RMSE") +
  theme_minimal()

# are metrics related to each other
# useful for assessing whether there are tradeoffs
#ggplot(results, aes(minannual_cor, rmse))+geom_point()

```

